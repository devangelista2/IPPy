{"ch_in": 1, "ch_out": 1, "middle_ch": [32, 64, 128], "n_layers_per_block": 2, "down_layers": ["ResDownBlock", "AttentionDownBlock"], "up_layers": ["AttentionUpBlock", "ResUpBlock"], "n_heads": 8, "final_activation": "relu"}